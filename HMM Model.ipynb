{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Components of a Hidden Markov Model:\n",
    "    1. States - POS\n",
    "    2. Observations - headlines\n",
    "    \n",
    "    Probablities required that relate the above two components:\n",
    "    1. Initial Probability: An initial probability distribution over states\n",
    "    2. Final Probability: A final probability distribution over states\n",
    "    3. Transition Probability: a matrix A with the probabilities from going from one state to another\n",
    "    4. Emission probability: a matrix B with the probabilities of an observation being generated from a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Importing Libraries --\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --Reading the dataset in --\n",
    "ds = pd.read_csv('india-news-headlines.csv') \n",
    "ds = ds.drop(columns=['publish_date', 'headline_category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[:10000]\n",
    "\n",
    "# -- Text Processing --\n",
    "tknzr = TweetTokenizer()\n",
    "ds[\"headline_text\"] = ds[\"headline_text\"].str.lower()\n",
    "ds[\"headline_text\"] = ds[\"headline_text\"].apply(lambda x: tknzr.tokenize(x))\n",
    "ds[\"headline_text\"] = ds[\"headline_text\"].apply(lambda word: [word for word in word if word.isalpha()])\n",
    "\n",
    "\n",
    "#Dropping rows that contain empty list\n",
    "drop_list = []\n",
    "for idx in range(0, len(ds)):\n",
    "    if(ds['headline_text'].loc[idx] == []):\n",
    "        drop_list.append(idx)\n",
    "\n",
    "ds = ds.drop(drop_list)\n",
    "ds = ds.reset_index(drop=True)\n",
    "\n",
    "ds['headline_tags'] = ds['headline_text'].apply(nltk.pos_tag)\n",
    "ds['headline_tags'] = ds['headline_tags'].apply(lambda x: [x[1] for x in x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Building Text and Tags Dictionaries --\n",
    "tag_freq = defaultdict(int)\n",
    "words_by_tag = defaultdict(list)\n",
    "emi_freq = defaultdict(list)\n",
    "\n",
    "\n",
    "for idx in range(0, len(ds)):\n",
    "    text = ds.iloc[idx]['headline_text']\n",
    "    tags = ds.iloc[idx]['headline_tags']\n",
    "\n",
    "    for index, tag in enumerate(tags):\n",
    "        tag_freq[tag] += 1\n",
    "        words_by_tag[tag].append(text[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- All headline text (observations) in a list --\n",
    "l = ds['headline_text'].tolist()\n",
    "headline_text_list = []\n",
    "for sublist in l:\n",
    "    for item in sublist:\n",
    "        headline_text_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Building Probability Matrices --\n",
    "tags_list = sorted(tag_freq.keys())\n",
    "tags_tuple = tuple(tags_list)\n",
    "\n",
    "trans_prob_matrix = (pd.DataFrame(columns=tags_list, index=tags_list)).fillna(0)\n",
    "initial_prob_matrix = pd.DataFrame(columns=['Count'], index=tags_list).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Building Initial Probability Matrix -- \n",
    "\n",
    "for idx in ds['headline_tags']:\n",
    "    initial_prob_matrix.loc[idx[0]] =  initial_prob_matrix.loc[idx[0]]+1\n",
    "initial_prob_matrix = initial_prob_matrix['Count'].divide(len(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Building Transition Probability Matrix -- \n",
    "import operator\n",
    "\n",
    "\n",
    "for tags in ds['headline_tags']: \n",
    "    for tag_idx in range(0, len(tags)-1):\n",
    "        trans_prob_matrix[tags[tag_idx+1]][tags[tag_idx]] = trans_prob_matrix[tags[tag_idx+1]][tags[tag_idx]]+1\n",
    "for index, row in trans_prob_matrix.iterrows():\n",
    "    trans_prob_matrix.loc[index] /= trans_prob_matrix.loc[index].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Building Nth Order Transition Probability Matrix -- \n",
    "\n",
    "# N = 2 for now\n",
    "\n",
    "import itertools\n",
    "import operator\n",
    "\n",
    "n_order_tags_list = []\n",
    "for tup in itertools.product(tags_list, repeat=2):\n",
    "    n_order_tags_list.append(tup[0]+'-'+tup[1])\n",
    "\n",
    "n_order_trans_prob_matrix = (pd.DataFrame(columns=tags_list, index=n_order_tags_list)).fillna(0)\n",
    "\n",
    "for tags in ds['headline_tags']: \n",
    "    for tag_idx in range(0, len(tags)-2):\n",
    "        n_order_trans_prob_matrix[tags[tag_idx+2]][tags[tag_idx]+'-'+tags[tag_idx+1]] = n_order_trans_prob_matrix[tags[tag_idx+2]][tags[tag_idx]+'-'+tags[tag_idx+1]]+1\n",
    "for index, row in n_order_trans_prob_matrix.iterrows():\n",
    "    if n_order_trans_prob_matrix.loc[index].sum() == 0:\n",
    "        n_order_trans_prob_matrix.drop([index])\n",
    "        n_order_tags_list.remove(index)\n",
    "    else:\n",
    "        n_order_trans_prob_matrix.loc[index] /= n_order_trans_prob_matrix.loc[index].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Supporting Data Structures --\n",
    "# -- word_freq = {'tag' : ['word1', 'word2'...]}\n",
    "\n",
    "all_words_l = []\n",
    "\n",
    "# Getting a list of all unique words in the headlines. This list will be used to get columns for the emission probability matrix\n",
    "for key in words_by_tag: #Get the list of words for each tag\n",
    "    l = words_by_tag[key]\n",
    "    u_e = sorted(set(l)) #Get unique words from that list\n",
    "    all_words_l.append(u_e)\n",
    "    words_by_tag[key] = u_e\n",
    "all_words_l = [item for sublist in all_words_l for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Building Emission Probability Matrix -- \n",
    "\n",
    "emission_prob_matrix = pd.DataFrame(columns=all_words_l, index=tags_list).fillna(0)\n",
    "\n",
    "for key in words_by_tag: #Get the list of words for each tag\n",
    "    l_words = words_by_tag[key]\n",
    "    for idx in range(0, len(l_words)):\n",
    "        word = l_words[idx]\n",
    "        emission_prob_matrix.loc[key][word] = emission_prob_matrix.loc[key][word]+1\n",
    "for index, row in emission_prob_matrix.iterrows():\n",
    "    emission_prob_matrix.loc[index] /= emission_prob_matrix.loc[index].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './HMM_DataStructures'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "pickle.dump( tags_list, open( \"./HMM_DataStructures/N.p\", \"wb\" ) )\n",
    "pickle.dump( ds[\"headline_text\"], open( \"./HMM_DataStructures/M.p\", \"wb\" ) )\n",
    "pickle.dump( initial_prob_matrix, open( \"./HMM_DataStructures/pi.p\", \"wb\" ) )\n",
    "pickle.dump( trans_prob_matrix, open( \"./HMM_DataStructures/A.p\", \"wb\" ) )\n",
    "pickle.dump(n_order_trans_prob_matrix, open( \"./HMM_DataStructures/A_2.p\", \"wb\" ) )\n",
    "pickle.dump( emission_prob_matrix, open( \"./HMM_DataStructures/B.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
